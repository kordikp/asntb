{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a4efd2-ac6f-4fb0-99ee-bcd7acf8a912",
   "metadata": {},
   "source": [
    "# The task\n",
    "\n",
    "The \"goodbooks\" dataset contains six million ratings for the ten\n",
    "thousand most popular books. It includes data such as books marked\n",
    "\"to-read\" by users, book metadata (author, year, etc.), and\n",
    "tags/shelves/genres. For this task, we would like you to cluster similar\n",
    "books by considering both interaction data (user ratings) and metadata.\n",
    "Although the dataset includes some metadata (e.g., author and title),\n",
    "feel free to collect additional data from the web or generate new data.\n",
    "Additionally, please propose a method to automatically label the\n",
    "clusters in a way that reflects their semantic content. We encourage\n",
    "creativity in this process and are not setting strict guidelines for\n",
    "clustering.\n",
    "\n",
    "Link to the dataset: https://github.com/zygmuntz/goodbooks-10k\n",
    "\n",
    "What to Deliver\n",
    "a) Presentation: A 10-minute presentation where you explain your solution.\n",
    "b) Demo: A demonstration showing the clusters in a low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34fb0f-2872-45ef-a07a-03f7c3d70612",
   "metadata": {},
   "source": [
    "# 1) Exploration\n",
    "\n",
    "Covered in the *quick_look.ipynb* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df488624-01f1-445c-a159-42bf6cdddc8e",
   "metadata": {},
   "source": [
    "# 2) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609b7f8d-5fc2-45d9-a07f-ac8acdec566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "859f64ae-f9ec-4325-8844-fc0a4c85887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "\n",
    "def load_goodbooks_data(base_path):\n",
    "    \"\"\"Load and prepare Goodbooks-10k dataset files with normalized tags, exclusions, and compound splitting.\"\"\"\n",
    "    \n",
    "    # Default manual mapping for common cases\n",
    "    default_mapping = {\n",
    "        # Children's books\n",
    "        'childrens': 'childrens',\n",
    "        'children': 'childrens',\n",
    "        'childrensbooks': 'childrens',\n",
    "        'kids': 'childrens',\n",
    "        'kidsbooks': 'childrens',\n",
    "        \n",
    "        # Young Adult\n",
    "        'youngadult': 'youngadult',\n",
    "        'ya': 'youngadult',\n",
    "        'newadult': 'youngadult',\n",
    "        \n",
    "        # Science Fiction\n",
    "        'scifi': 'scifi',\n",
    "        'sciencefiction': 'scifi',\n",
    "\n",
    "        # Classics\n",
    "        'classics': 'classics',\n",
    "        'classic': 'classics',        \n",
    "\n",
    "        # Humour\n",
    "        'humour': 'humour',\n",
    "        'humor': 'humour',\n",
    "\n",
    "        # History\n",
    "        'history': 'history',\n",
    "        'historical': 'history',\n",
    "        'historic': 'history',\n",
    "\n",
    "        # Audiobook\n",
    "        'audiobook': 'audiobook',\n",
    "        'audiobooks': 'audiobook',\n",
    "        'audio': 'audiobook',\n",
    "        'audible': 'audiobook',\n",
    "\n",
    "        # Ebook\n",
    "        'ebook': 'ebook',\n",
    "        'ebooks': 'ebook',\n",
    "        'kindle': 'ebook',\n",
    "\n",
    "        # Favourite\n",
    "        'favourite': 'favourite',\n",
    "        'favourites': 'favourite',\n",
    "        'favorite': 'favourite',\n",
    "        'favorites': 'favourite',\n",
    "        'alltimefavorites': 'favourite',\n",
    "        'favoritebooks': 'favourite',\n",
    "        'shelfarifavorites': 'favourite',\n",
    "\n",
    "        # Graphicnovel\n",
    "        'graphicnovel': 'graphicnovel',\n",
    "        'graphicnovels': 'graphicnovel',\n",
    "\n",
    "        # Memoir\n",
    "        'memoir': 'memoir',\n",
    "        'memoirs': 'memoir',\n",
    "\n",
    "        # Fiction\n",
    "        'fiction': 'fiction',\n",
    "        'generalfiction': 'fiction',\n",
    "        'literaryfiction': 'fiction',\n",
    "        'toreadfiction': 'fiction',\n",
    "\n",
    "        # Fantasy\n",
    "        'fantasy': 'fantasy',\n",
    "        'epicfantasy': 'fantasy',\n",
    "\n",
    "        # Biography\n",
    "        'biography': 'biography',\n",
    "        'biographies': 'biography',\n",
    "    }\n",
    "    \n",
    "    # Default compound tags to split\n",
    "    default_compound_splits = {\n",
    "        'historicalfiction': ['history', 'fiction'],\n",
    "        'adultfiction': ['adult', 'fiction'],\n",
    "        'scififantasy': ['scifi', 'fantasy'],\n",
    "        'fantasyscifi': ['scifi', 'fantasy'],\n",
    "        'paranormalromance': ['paranormal', 'romance'],\n",
    "        'contemporaryromance': ['contemporary', 'romance'],\n",
    "        'historicalromance': ['history', 'romance'],\n",
    "        'urbanfantasy': ['urban', 'fantasy'],\n",
    "        'literaryfiction': ['literary', 'fiction'],\n",
    "        'romanticfantasy': ['romance', 'fantasy'],\n",
    "        'psychologicalthriller': ['psychological', 'thriller'],\n",
    "        'crimefiction': ['crime', 'fiction'],\n",
    "        'darkfantasy': ['dark', 'fantasy'],\n",
    "        'mysterythriller': ['mystery', 'thriller'],\n",
    "        'contemporaryfiction': ['contemporary', 'fiction'],\n",
    "        'christianfiction': ['christian', 'fiction'],\n",
    "        'yafiction': ['youngadult', 'fiction'],\n",
    "        'audiobiography': ['audio', 'biography'],\n",
    "    }\n",
    "\n",
    "    # Default tags to exclude    \n",
    "    default_exclude_tags = {\n",
    "        'books', 'literature', 'default', 'favourite', # general\n",
    "        'toread', 'currentlyreading', 'readin', 'read', 'reads', 'reread', # read related\n",
    "        'owned', 'booksiown', 'ownedbooks', 'iown', 'ownit', 'have', # own related\n",
    "        'library', 'bookclub', 'mylibrary', 'mybooks', # library related\n",
    "        'tobuy', 'wishlist', # buy related\n",
    "        'maybe', 'borrowed', 'thcentury', '', 'dnf', 'sf', 's', #others\n",
    "        'audiobook', 'ebook', #format\n",
    "        'didnotfinish', 'unfinished', 'didntfinish', 'finished', # finish related\n",
    "    }\n",
    "    \n",
    "    def normalize_and_split_tag(tag):\n",
    "        \"\"\"Normalize tag names and handle compound splitting.\"\"\"\n",
    "        # Basic normalization\n",
    "        tag = str(tag)\n",
    "        tag = tag.replace('-', '')\n",
    "        tag = ''.join([c for c in tag if not c.isdigit()])\n",
    "        tag = ''.join(tag.split()).lower()\n",
    "        \n",
    "        # Check if it's a compound tag that should be split\n",
    "        if tag in default_compound_splits:\n",
    "            return default_compound_splits[tag]\n",
    "        \n",
    "        # If not a compound tag, apply regular mapping\n",
    "        return [default_mapping.get(tag, tag)]\n",
    "    \n",
    "    # Load the data\n",
    "    books = pd.read_csv(f\"{base_path}/books.csv\")\n",
    "    ratings = pd.read_csv(f\"{base_path}/ratings.csv\")\n",
    "    tags = pd.read_csv(f\"{base_path}/book_tags.csv\")\n",
    "    tag_names = pd.read_csv(f\"{base_path}/tags.csv\")\n",
    "    \n",
    "    # Store original statistics\n",
    "    original_tag_count = len(tag_names['tag_name'].unique())\n",
    "    \n",
    "    # Process tags with splitting\n",
    "    expanded_tags = []\n",
    "    for _, row in tags.merge(tag_names, on='tag_id').iterrows():\n",
    "        split_tags = normalize_and_split_tag(row['tag_name'])\n",
    "        for tag in split_tags:\n",
    "            if tag not in default_exclude_tags:  # Only add if not excluded\n",
    "                expanded_tags.append({\n",
    "                    'goodreads_book_id': row['goodreads_book_id'],\n",
    "                    'tag_name': tag,\n",
    "                    'count': row['count']\n",
    "                })\n",
    "    \n",
    "    # Create new DataFrame with expanded tags\n",
    "    tags = pd.DataFrame(expanded_tags)\n",
    "    \n",
    "    # Aggregate counts for same tags\n",
    "    tags = tags.groupby(['goodreads_book_id', 'tag_name'])['count'].sum().reset_index()\n",
    "    \n",
    "    # Normalize tag counts\n",
    "    tags['count_normalized'] = tags.groupby('goodreads_book_id')['count'].transform(\n",
    "        lambda x: x / x.sum()\n",
    "    )\n",
    "    \n",
    "    # Set index for books DataFrame\n",
    "    books.set_index('book_id', inplace=True)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nFinal tag statistics:\")\n",
    "    print(f\"Original number of unique tags: {original_tag_count}\")\n",
    "    print(f\"Number of unique tags after processing: {len(tags['tag_name'].unique())}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    return books, ratings, tags\n",
    "\n",
    "\n",
    "def preprocess_ratings(ratings_df, books_df, n_components=100):\n",
    "    \"\"\"Process ratings and reduce their dimensionality.\"\"\"\n",
    "    \n",
    "    # Create user-item matrix\n",
    "    user_item_matrix = ratings_df.pivot(\n",
    "        index='book_id', \n",
    "        columns='user_id', \n",
    "        values='rating'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Add missing books with zero ratings\n",
    "    user_item_matrix = user_item_matrix.reindex(books_df.index, fill_value=0)\n",
    "    \n",
    "    # Compute latent factors\n",
    "    svd = TruncatedSVD(n_components=min(n_components, min(user_item_matrix.shape) - 1))\n",
    "    latent_factors = svd.fit_transform(user_item_matrix)\n",
    "    \n",
    "    # Calculate rating statistics for all books\n",
    "    rating_stats = pd.DataFrame(index=books_df.index)\n",
    "    \n",
    "    # For books with ratings\n",
    "    rating_stats['rating_mean'] = ratings_df.groupby('book_id')['rating'].mean()\n",
    "    rating_stats['rating_std'] = ratings_df.groupby('book_id')['rating'].std()\n",
    "    rating_stats['rating_count'] = ratings_df.groupby('book_id')['rating'].count()\n",
    "    \n",
    "    # Fill missing values\n",
    "    rating_stats['rating_mean'] = rating_stats['rating_mean'].fillna(0)\n",
    "    rating_stats['rating_std'] = rating_stats['rating_std'].fillna(0)\n",
    "    rating_stats['rating_count'] = rating_stats['rating_count'].fillna(0)\n",
    "    \n",
    "    return latent_factors, rating_stats\n",
    "\n",
    "\n",
    "def process_metadata(books_df, tags_df, n_tags=200, n_tag_components=20, n_authors=200, n_author_components=20):\n",
    "    \"\"\"Process book metadata features with proper dimensionality reduction for sparse matrices.\"\"\"\n",
    "    \n",
    "    # Process tags\n",
    "    tag_agg = tags_df.groupby(['goodreads_book_id', 'tag_name'])['count_normalized'].sum().reset_index()\n",
    "    \n",
    "    # Get top n most common tags\n",
    "    top_tags = tag_agg.groupby('tag_name')['count_normalized'].sum().nlargest(n_tags).index\n",
    "    tag_agg_filtered = tag_agg[tag_agg['tag_name'].isin(top_tags)]\n",
    "    \n",
    "    # Create tag matrix and fill missing books with zeros\n",
    "    tag_matrix = tag_agg_filtered.pivot(\n",
    "        index='goodreads_book_id',\n",
    "        columns='tag_name',\n",
    "        values='count_normalized'\n",
    "    )\n",
    "    tag_matrix = tag_matrix.reindex(books_df.index, fill_value=0)\n",
    "    \n",
    "    # Fill any remaining NaN values with 0 before SVD\n",
    "    tag_matrix = tag_matrix.fillna(0)\n",
    "    \n",
    "    # Reduce dimensionality of tag matrix using TruncatedSVD\n",
    "    tag_svd = TruncatedSVD(n_components=n_tag_components)  # Adjust number of components as needed\n",
    "    tag_reduced = tag_svd.fit_transform(tag_matrix)\n",
    "    #tag_variance = sum(tag_svd.explained_variance_ratio_)\n",
    "    #print(f\"Variance explained by tag SVD: {tag_variance:.2%}\")\n",
    "    \n",
    "    # Convert to DataFrame and scale\n",
    "    tag_matrix_reduced = pd.DataFrame(\n",
    "        tag_reduced,\n",
    "        index=tag_matrix.index,\n",
    "        columns=[f'tag_component_{i}' for i in range(tag_reduced.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Process authors\n",
    "    books_df['authors'] = books_df['authors'].fillna('Unknown Author')\n",
    "    \n",
    "    # Get top n authors (and take into account that book may have multiple authors)\n",
    "    author_counts = books_df['authors'].str.split(',').explode().value_counts()\n",
    "    top_authors = author_counts.nlargest(n_authors).index\n",
    "    \n",
    "    # Create author features\n",
    "    authors = pd.get_dummies(\n",
    "        books_df['authors'].str.split(',').explode()\n",
    "    ).groupby(level=0).sum()\n",
    "    authors = authors[authors.columns[authors.columns.isin(top_authors)]]\n",
    "    authors = authors.reindex(books_df.index, fill_value=0)\n",
    "    \n",
    "    # Fill any remaining NaN values with 0 before SVD\n",
    "    authors = authors.fillna(0)\n",
    "    \n",
    "    # Reduce dimensionality of author matrix using TruncatedSVD\n",
    "    author_svd = TruncatedSVD(n_components=n_author_components)\n",
    "    author_reduced = author_svd.fit_transform(authors)\n",
    "    #author_variance = sum(author_svd.explained_variance_ratio_)\n",
    "    #print(f\"Variance explained by author SVD: {author_variance:.2%}\")\n",
    "    \n",
    "    # Convert to DataFrame and scale\n",
    "    authors_reduced = pd.DataFrame(\n",
    "        author_reduced,\n",
    "        index=authors.index,\n",
    "        columns=[f'author_component_{i}' for i in range(author_reduced.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Process numerical features\n",
    "    numerical_cols = ['average_rating', 'ratings_count', 'work_ratings_count', 'work_text_reviews_count', 'original_publication_year']\n",
    "    numerical_features = books_df[numerical_cols].copy()\n",
    "    \n",
    "    # Fill missing values with sensible defaults\n",
    "    defaults = {\n",
    "        'average_rating': numerical_features['average_rating'].median(),\n",
    "        'ratings_count': 0,\n",
    "        'work_ratings_count': 0,\n",
    "        'work_text_reviews_count': 0,\n",
    "        'original_publication_year': numerical_features['original_publication_year'].median()\n",
    "    }\n",
    "    \n",
    "    for col, default in defaults.items():\n",
    "        numerical_features[col] = numerical_features[col].fillna(default)\n",
    "    \n",
    "    # Log transform count-based features before scaling\n",
    "    log_columns = ['ratings_count', 'work_ratings_count', 'work_text_reviews_count']\n",
    "    for col in log_columns:\n",
    "        numerical_features[col] = np.log1p(numerical_features[col])\n",
    "    \n",
    "    return tag_matrix_reduced, authors_reduced, numerical_features\n",
    "\n",
    "\n",
    "def combine_features(latent_factors, rating_stats, tag_matrix, authors, numerical_features, active_features):\n",
    "    \"\"\"Combine features with 3D reduction.\"\"\"\n",
    "    \n",
    "    # Only process active features\n",
    "    base_features = {\n",
    "        'latent_factors': latent_factors if isinstance(latent_factors, pd.DataFrame) \n",
    "            else pd.DataFrame(latent_factors, index=rating_stats.index, \n",
    "                columns=[f'latent_factor_{i}' for i in range(latent_factors.shape[1])]),\n",
    "        'rating_stats': rating_stats,\n",
    "        'tags': tag_matrix,\n",
    "        'authors': authors,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "    \n",
    "    active_features = {\n",
    "        name: df for name, df in base_features.items() \n",
    "        if name in active_features and active_features[name] > 0\n",
    "    }\n",
    "\n",
    "    # Combine features\n",
    "    combined = pd.concat(active_features, axis=1)\n",
    "    combined = combined.fillna(0)\n",
    "\n",
    "    # Scale numerical features using RobustScaler for better handling of outliers\n",
    "    scaler = RobustScaler()\n",
    "    combined = pd.DataFrame(\n",
    "            scaler.fit_transform(combined),\n",
    "            index=combined.index,\n",
    "            columns=combined.columns\n",
    "    )\n",
    "    \n",
    "    # Use UMAP with 3 components\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        min_dist=0.2,\n",
    "        n_components=3,\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    \n",
    "    reduced_features = reducer.fit_transform(combined)\n",
    "    return reduced_features, combined, combined.index\n",
    "\n",
    "\n",
    "def cluster_books(features, min_clusters=5, max_clusters=30, step=1):\n",
    "    \"\"\"Perform clustering with improved parameter selection.\"\"\"\n",
    "    scores = []\n",
    "    kmeans_models = []\n",
    "    \n",
    "    for k in range(min_clusters, max_clusters + 1, step):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,\n",
    "            init='k-means++',\n",
    "            n_init=15\n",
    "        )\n",
    "        clusters = kmeans.fit_predict(features)\n",
    "        \n",
    "        # Compute multiple clustering metrics\n",
    "        sil_score = silhouette_score(features, clusters)\n",
    "        db_score = davies_bouldin_score(features, clusters)\n",
    "        \n",
    "        # Combine metrics (higher is better)\n",
    "        combined_score = sil_score - db_score\n",
    "        scores.append(combined_score)\n",
    "        kmeans_models.append(kmeans)\n",
    "    \n",
    "    best_model = kmeans_models[np.argmax(scores)]\n",
    "    final_clusters = best_model.fit_predict(features)\n",
    "    \n",
    "    return final_clusters\n",
    "\n",
    "\n",
    "def get_distinctive_cluster_labels(books_df, clusters, tags_df, cluster_index):\n",
    "    \"\"\"Generate meaningful labels for clusters based on filtered tags.\"\"\"\n",
    "    \n",
    "    clustered_books = books_df.loc[cluster_index].copy()\n",
    "    clustered_books['cluster'] = clusters\n",
    "    \n",
    "    def get_cluster_theme(cluster_id):\n",
    "        \"\"\"Get the main theme for a cluster using both titles and tags.\"\"\"\n",
    "        # Get books in this cluster\n",
    "        cluster_books = clustered_books[clustered_books['cluster'] == cluster_id]\n",
    "        \n",
    "        # Analyze tags\n",
    "        cluster_tags = tags_df[tags_df['goodreads_book_id'].isin(cluster_books.index)]\n",
    "        top_tags = cluster_tags.groupby('tag_name')['count'].sum().nlargest(10)\n",
    "\n",
    "        if len(top_tags) > 0:\n",
    "            primary_tag = top_tags.nlargest(1).index[0]\n",
    "        else:\n",
    "            primary_tag = 'Unknown'\n",
    "\n",
    "        if len(top_tags) > 1:\n",
    "            top_tags = top_tags[1:]\n",
    "            exclude_generic = ['fiction', 'classics']\n",
    "            filtered_top_tags = top_tags[~top_tags.index.str.contains('|'.join(exclude_generic), case=False)]\n",
    "            secondary_tag = filtered_top_tags.nlargest(1).index[0]\n",
    "        else:\n",
    "            secondary_tag = 'Unknown'\n",
    "        \n",
    "        # Get time period\n",
    "        years = cluster_books['original_publication_year']\n",
    "        median_year = years.median()\n",
    "        \n",
    "        # Construct label\n",
    "        label_parts = []\n",
    "        label_parts.append(primary_tag.title())\n",
    "        label_parts.append(secondary_tag.title())\n",
    "        label_parts.append(str(median_year))\n",
    "        \n",
    "        return \" \".join(label_parts)\n",
    "    \n",
    "    # Generate labels for all clusters\n",
    "    cluster_labels = {}\n",
    "    for cluster_id in range(clusters.max() + 1):\n",
    "        cluster_labels[cluster_id] = get_cluster_theme(cluster_id)\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "    \n",
    "def visualize_clusters_3d_with_labels(reduced_features, clusters, books_df, cluster_index, cluster_labels):\n",
    "    \"\"\"Create 3D visualization with automatic cluster labels.\"\"\"\n",
    "    \n",
    "    viz_df = pd.DataFrame(\n",
    "        reduced_features, \n",
    "        columns=['UMAP1', 'UMAP2', 'UMAP3'],\n",
    "        index=cluster_index\n",
    "    )\n",
    "    viz_df['Cluster'] = clusters\n",
    "    viz_df['Title'] = books_df.loc[cluster_index, 'title']\n",
    "    viz_df['Author'] = books_df.loc[cluster_index, 'authors']\n",
    "    viz_df['Rating'] = books_df.loc[cluster_index, 'average_rating']\n",
    "    \n",
    "    # Add semantic labels\n",
    "    cluster_sizes = pd.Series(clusters).value_counts()\n",
    "    viz_df['Cluster_Label'] = viz_df['Cluster'].apply(\n",
    "        lambda x: f\"{cluster_labels[x]} (n={cluster_sizes[x]})\"\n",
    "    )\n",
    "    \n",
    "    # Create interactive 3D scatter plot\n",
    "    fig = px.scatter_3d(\n",
    "        viz_df,\n",
    "        x='UMAP1',\n",
    "        y='UMAP2',\n",
    "        z='UMAP3',\n",
    "        color='Cluster_Label',\n",
    "        hover_data=['Title', 'Author', 'Rating'],\n",
    "        title='3D Book Clusters Visualization with Semantic Labels',\n",
    "        color_discrete_sequence=px.colors.qualitative.Set3,\n",
    "        opacity=0.6\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='UMAP Dimension 1',\n",
    "            yaxis_title='UMAP Dimension 2',\n",
    "            zaxis_title='UMAP Dimension 3'\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "    \n",
    "# Update the main function to include automatic labeling\n",
    "def main(base_path, active_features=None):\n",
    "    \"\"\"Main function with automatic cluster labeling.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    books, ratings, tags = load_goodbooks_data(base_path)\n",
    "    \n",
    "    # Process ratings data\n",
    "    latent_factors, rating_stats = preprocess_ratings(ratings, books)\n",
    "    \n",
    "    # Process metadata including original title features\n",
    "    tag_matrix, authors, numerical_features = process_metadata(books, tags)\n",
    "    \n",
    "    # Combine features\n",
    "    reduced_features, combined_features, common_index = combine_features(\n",
    "        latent_factors,\n",
    "        rating_stats,\n",
    "        tag_matrix,\n",
    "        authors,\n",
    "        numerical_features,\n",
    "        active_features\n",
    "    )\n",
    "    \n",
    "    # Perform clustering\n",
    "    clusters = cluster_books(reduced_features)\n",
    "    print(f\"\\n\\n\\nNumber of clusters: {len(np.unique(clusters))}\")\n",
    "    \n",
    "    # Generate cluster labels\n",
    "    cluster_labels = get_distinctive_cluster_labels(books, clusters, tags, common_index)\n",
    "    \n",
    "    # Create visualizations with semantic labels\n",
    "    cluster_3d = visualize_clusters_3d_with_labels(\n",
    "        reduced_features, clusters, books, \n",
    "        common_index, cluster_labels\n",
    "    )\n",
    "    \n",
    "    # Print cluster descriptions\n",
    "    print(\"\\nCluster Descriptions:\")\n",
    "    for cluster_id, label in cluster_labels.items():\n",
    "        size = (clusters == cluster_id).sum()\n",
    "        print(f\"\\nCluster {cluster_id} ({size} books): {label}\")\n",
    "    \n",
    "    return books, reduced_features, clusters, {\n",
    "        'cluster_3d': cluster_3d,\n",
    "    }, common_index, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ae4f5d-64d4-445d-9a7a-568331ea8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final tag statistics:\n",
      "Original number of unique tags: 34252\n",
      "Number of unique tags after processing: 31627\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 1,\n",
    "    'rating_stats': 0,\n",
    "    'tags': 1,\n",
    "    'authors': 1,\n",
    "    'numerical': 0,\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7b189e4-60be-4867-9034-a38e5a89c6e1",
   "metadata": {},
   "source": [
    "In this final solution, we can see fairly meaningful clusters. A few examples that should be familiar even to someone who isn't interested in literature: cluster 1 contains the Harry Potter series and similar books, cluster 4 literally looks like a list of required classic literature for high school, cluster 10 contains mainly self-help books, cluster 12 horror books, cluster 13 children's books and cluster 19 manga.\n",
    "\n",
    "The most challening part was evaluation. While it is fairly straight forward to mathematically evaluate the shape and density, itâ€™s quite complicated to mathematically express whether the clusters make semantically sense. Therefore, it was necessary to manually evaluate every run.\n",
    "\n",
    "*PS: The TF-IDF experiments with the title feature mentioned in the presentation were removed, as they were increasing the computational complexity while not improving the results at all.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c52c22-5ff9-4ed9-bedb-e3f70839a605",
   "metadata": {},
   "source": [
    "# 3) Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7116030-7c49-4a72-ad0e-9ea2f24a4090",
   "metadata": {},
   "source": [
    "In the previous part, we already tried tag based labelling. However, the most common tags are not very informative. Therefore, we will experiment with LLM based labelling. I will choose a sample of books from each cluster and prompt ChatGPT via API to reply with a brief description and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951649b-daf6-420f-bbaf-f7b4d3b0eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==0.28\n",
    "# !pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e3da0-60e4-4cec-8f70-b863c5618e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_strings = []\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    example_books = books_in_cluster.nlargest(10, 'ratings_count')\n",
    "    books_string = [f\"{row['title']} by {row['authors']}\" for _, row in example_books.iterrows()]\n",
    "    prompt_strings.append(\", \".join(books_string))\n",
    "\n",
    "# prompt_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269af77b-0836-4ec3-a536-bd612a8d27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"placeholder\"\n",
    "\n",
    "# Function to communicate with the API\n",
    "def get_llm_description(prompt, model=\"gpt-3.5-turbo\", temperature=0.15, max_tokens=200):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aff72d-cd3f-4371-8db4-2725411821d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster descriptions\n",
    "cluster_descriptions = []\n",
    "prompt_instruction = \"I will give you the names of 10 books and your task is to concisely describe the group of books. Do not include the name of any of the authors. Do not include the name of any of the authors! Describe using genres or period when they were written that describes majority of them. Use 8 words at maximum per group, not per book. Here is the group: \"\n",
    "for books_in_cluster in prompt_strings:\n",
    "    response = get_llm_description(prompt_instruction + books_in_cluster)\n",
    "    cluster_descriptions.append(response)\n",
    "\n",
    "# print(cluster_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d8ecd-23ae-43f2-8237-9f438ad30048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster labels\n",
    "cluster_labels = []\n",
    "prompt_instruction = \"I will give you the names of 10 books and your task is to concisely assign a label to the group. The label should be based on the genre of majority of the books in the group. Use 2 words at maximum per group, not per book. Here is the group: \"\n",
    "for books_in_cluster in prompt_strings:\n",
    "    response = get_llm_description(prompt_instruction + books_in_cluster)\n",
    "    cluster_labels.append(response)\n",
    "\n",
    "# print(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517462ac-84cc-48a0-96e3-293434d41e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for i, (cluster_id, label) in enumerate(labels.items()):\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {cluster_labels[i]} - {cluster_descriptions[i]}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d02ef-b30c-4992-b2c1-e142fcffa516",
   "metadata": {},
   "source": [
    "The LLM generated tags and descriptions seem to be much more descriptive than the tag based ones, which actually makes the manual evaluation of semantic quality of the clusters a bit easier. A bit of prompt engineering might improve them a bit, but I'm satisfied with these and will call this my final solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53385fb-ed9c-461a-a75a-72ebe16b779a",
   "metadata": {},
   "source": [
    "# Archive of some auxiliary experiments\n",
    "## Determining the number of components in relationships SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b48998-b6d6-45ee-8e32-a02605c028ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df, ratings_df, _ = load_goodbooks_data(base_path)\n",
    "\n",
    "# Create user-item matrix\n",
    "user_item_matrix = ratings_df.pivot(\n",
    "    index='book_id', \n",
    "    columns='user_id', \n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "# Add missing books with zero ratings\n",
    "user_item_matrix = user_item_matrix.reindex(books_df.index, fill_value=0)\n",
    "\n",
    "# Method 1: Calculate explained variance ratio\n",
    "svd = TruncatedSVD(n_components=1000)\n",
    "svd.fit(user_item_matrix)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "cumulative_variance_ratio = np.cumsum(svd.explained_variance_ratio_)\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4470c5-860b-430d-8387-5e2fc14a5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of components for desired explained variance\n",
    "target_variance = 0.5\n",
    "n_components = np.argmax(cumulative_variance_ratio >= target_variance) + 1\n",
    "print(f\"Number of components needed for {target_variance*100}% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21677aa6-0b0f-4baa-86fa-752e7b1f7aa0",
   "metadata": {},
   "source": [
    "## One feature at a time testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f35ae-dab6-44d9-a177-df52e06c2027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 1,\n",
    "    'rating_stats': 0,\n",
    "    'tags': 0,\n",
    "    'authors': 0,\n",
    "    'numerical': 0\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948666f-bafd-402e-a562-0cb21780aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 0,\n",
    "    'rating_stats': 1,\n",
    "    'tags': 0,\n",
    "    'authors': 0,\n",
    "    'numerical': 0\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154feb33-e418-40b9-aef9-49996967ecfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 0,\n",
    "    'rating_stats': 0,\n",
    "    'tags': 1,\n",
    "    'authors': 0,\n",
    "    'numerical': 0\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c99b7a-45d7-4860-80ea-fc2b2a493eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 0,\n",
    "    'rating_stats': 0,\n",
    "    'tags': 0,\n",
    "    'authors': 1,\n",
    "    'numerical': 0,\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca366bc-2dcf-49af-8d0a-31e4370db03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_features = {\n",
    "    'latent_factors': 0,\n",
    "    'rating_stats': 0,\n",
    "    'tags': 0,\n",
    "    'authors': 0,\n",
    "    'numerical': 1,\n",
    "}\n",
    "\n",
    "base_path = \".\"\n",
    "books, reduced_features, clusters, plots, common_index, labels = main(base_path, active_features)\n",
    "plots['cluster_3d'].show()\n",
    "\n",
    "# Print detailed cluster descriptions with example books\n",
    "for cluster_id, label in labels.items():\n",
    "    books_in_cluster = books.loc[common_index][clusters == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(\"Example books:\")\n",
    "    print(books_in_cluster.nlargest(10, 'ratings_count')[['title', 'authors']].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
